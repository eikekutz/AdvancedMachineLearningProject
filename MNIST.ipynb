{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Tom_uncluttered.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"PiJASRStjYwP","colab_type":"text"},"source":["# A locally adaptive normal Distribution"]},{"cell_type":"markdown","metadata":{"id":"u7nBWG8RjY-N","colab_type":"text"},"source":["### Imports"]},{"cell_type":"code","metadata":{"id":"ksWIee5ajbPP","colab_type":"code","outputId":"7ec43822-8092-40f0-e23d-a02a9adf7d50","executionInfo":{"status":"ok","timestamp":1557752967354,"user_tz":-120,"elapsed":6559,"user":{"displayName":"Tom Haider","photoUrl":"","userId":"04727568713436086022"}},"colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["colab = True\n","\n","if colab:\n","    from google.colab import drive\n","    drive.mount('/content/gdrive')\n","\n","    import os\n","    os.chdir('/content/gdrive/My Drive/02460AdvancedML/Code/')\n","\n","    #!git clone https://github.com/rtqichen/torchdiffeq.git\n","    !pip install -e torchdiffeq\n","    \n","import torch\n","import numpy as np\n","from scipy.io import loadmat#'''##'''\n","from land.curve import *\n","from land.geodesics import *\n","from land.local_pca_metric import *\n","from IPython.display import Image\n","import random\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import seaborn as sns\n","import math\n","import sys\n","from time import perf_counter\n","import multiprocessing\n","from functools import partial\n","from contextlib import contextmanager"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Obtaining file:///content/gdrive/My%20Drive/02460AdvancedML/Code/torchdiffeq\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from torchdiffeq==0.0.1) (1.1.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->torchdiffeq==0.0.1) (1.16.3)\n","Installing collected packages: torchdiffeq\n","  Found existing installation: torchdiffeq 0.0.1\n","    Can't uninstall 'torchdiffeq'. No files were found to uninstall.\n","  Running setup.py develop for torchdiffeq\n","Successfully installed torchdiffeq\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"giqrKJQPkaDs","colab_type":"code","colab":{}},"source":["def geodesic_minimizing_energy(curve, manifold, optimizer=torch.optim.LBFGS, max_iter=150, eval_grid=20):\n","    \"\"\"\n","    Compute a geodesic curve connecting two points by minimizing its energy.\n","\n","    Mandatory inputs:\n","    curve:      a curve object representing a curve with fixed end-points.\n","                When the function returns, this object has been updated to\n","                be a geodesic curve.\n","    manifold:   a manifold object representing the space over which the\n","                geodesic is defined. This object must provide a\n","                'curve_energy' function through which pytorch can\n","                back-propagate.\n","    \n","    Optional inputs:\n","    optimizer=torch.optim.LBFGS:    choice of iterative optimizer.\n","    max_iter=150:                   max number of iterations of the optimizer.\n","    eval_grid=20:                   number of points along the curve where\n","                                    energy is evaluated.\n","    \"\"\"\n","    ## Initialize optimizer and set up closure\n","    alpha = torch.linspace(0, 1, eval_grid).reshape((-1, 1))\n","    opt = optimizer([curve.parameters])\n","    def closure():\n","        opt.zero_grad()\n","        loss = manifold.curve_energy(curve(alpha))\n","        loss.backward()\n","        return loss\n","\n","    for iter in range(max_iter):\n","        opt.step(closure=closure)\n","        if torch.max(torch.abs(curve.parameters.grad)) < 1e-4:\n","            break\n","\n","def shooting_geodesic(manifold, p, v, t=torch.linspace(0, 1, 50), requires_grad=False):\n","    if requires_grad:\n","        from torchdiffeq import odeint_adjoint as odeint\n","    else:\n","        from torchdiffeq import odeint\n","    odefunc = GeodesicODE(manifold)\n","    y = torch.cat([p.reshape(-1, 1), v.reshape(-1, 1)], dim=0) # (2D)xN\n","    retval = odeint(odefunc, y, t) # Tx(2D)xN\n","    D = retval.shape[1]//2\n","    c = retval[:, :D, :] # TxDxN\n","    dc = retval[:, D:, :] # TxDxN\n","    return c, dc"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sIpYV1qkkOZ1","colab_type":"text"},"source":["### Construct the metric"]},{"cell_type":"code","metadata":{"id":"E7dMtfgdkNhd","colab_type":"code","colab":{}},"source":["## Read data\n","data = torch.tensor(loadmat('data/mnist1000.mat')['data'], dtype=torch.float)\n","N, D = data.shape  #torch.Size([1000, 2])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OrPLse0EkVqJ","colab_type":"code","colab":{}},"source":["## Parameters for metric\n","sigma = 0.1 #0.1\n","rho = 0.1 #0.1\n","\n","## Create metric\n","M = LocalVarMetric(data=data, sigma=sigma, rho=rho)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KmjX-vdcdPtn","colab_type":"text"},"source":["### Glossaray and Notations"]},{"cell_type":"markdown","metadata":{"id":"IuPQIyjkdQC9","colab_type":"text"},"source":["|Symbol  $\\qquad$ $\\qquad$ |              Explenation  |\n","|-|-|\n","|$\\mathcal{C}$| Normalization Constant        |\n","|$D$| Dimensionality of the Data|\n","|$\\mathrm{Exp}_x (v)$ | takes a tangent vector  $\\mathbf{v} \\in \\mathcal{T} _x \\mathcal{M}$ such that the curve $\\gamma (t) =\\mathrm{Exp}_x (t \\cdot v)$ is a geodesic originatiing at x with initial velocity $\\mathbf{v}$ and length $||\\mathbf{v}||$   |\n","|$\\gamma$| metric tensor |\n","|$\\gamma'$| tangent vector, derivaitve (velocity) of $\\gamma$ |\n","|$\\mathrm{Log}_x (y) $ | inverse mapping, gives the initial tangent vector at point x|\n","|$\\mathbf{\\mu} $| Mean of a Gaussian Distribution |\n","|$\\mathcal{M}$| Riemannian manifold |\n","|$M$| Riemannian Metric|\n","|$p_{\\mathcal{M}} (\\mathbf{x}|\\mu, \\mathbf{\\Sigma} )$| Riemannian Normal distribution|\n","|$\\phi$| 'data fitting term'|\n","|$\\rho$| regularization parameter to avoid singular covariances|\n","|$\\mathcal{S}_{++}^D$| Set of symmetric $D \\times D$ positive definite matrices|\n","|$\\mathbf{\\Sigma}$ | Covariance Matrix |\n","|$\\nabla$| Gradient of $\\mu$|\n","|$\\mathcal{T} _x \\mathcal{M}$| tangent space at $x \\in \\mathcal{M}$|\n","|$\\mathbf{v}$| Vector at a point in direction of geodesic to another point|\n","|$\\mathbf{x}$ | random vector $\\mathbf{x} \\in \\mathbb{R}^D $ (e.g. a point in the data)\n","|$x_{nd}$| $d^{th}$ dimension of the $n^{th} $ observation|"]},{"cell_type":"markdown","metadata":{"id":"v4IHzWALfcBm","colab_type":"text"},"source":["### Calculating $\\mathrm{Exp}_p(V)$ "]},{"cell_type":"markdown","metadata":{"id":"xoKOuFu0htCT","colab_type":"text"},"source":["Calculate $Exp_p(v)$ where p is the starting point and v is the velocity\n","\n","use the function shooting-geodesic to calculate the necessary points\n","take last datapoint of the estimated curve\n","\n","$q = Exp_p(v)$ where q is the end point\n","\n","$\\gamma' = dc = derivative of \\gamma = velocity-on-each-point-in-c$"]},{"cell_type":"code","metadata":{"id":"zPWxocR1fbW2","colab_type":"code","colab":{}},"source":["def ExpMap(p,v):\n","    c = shooting_geodesic(M,p,v)[0] \n","    return c[-1].reshape(1, D)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KAxRybe8gC0c","colab_type":"text"},"source":["### Calculating $\\mathrm{Log}_p(q)$\n"]},{"cell_type":"markdown","metadata":{"id":"3pX6p-kihjaD","colab_type":"text"},"source":["Calculate $\\mathrm{Log}_p(q)$ where p is the starting point and q is the end point\n","\n","use the function geodesic_minimize_energy to calculate the initial vector at the starting point\n","\n","$v = Log_x(y)$"]},{"cell_type":"code","metadata":{"id":"PefwbXCUgDIS","colab_type":"code","colab":{}},"source":["def LogMap(p,q, num_nodes=10):\n","    C = CubicSpline(begin=p, end=q, num_nodes=num_nodes, requires_grad=True)\n","    geodesic_minimizing_energy(C, M)\n","    t = torch.linspace(0, 1, 2, dtype=C.begin.dtype)\n","    return C.deriv(t)[0].detach(),C"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dQyQd0Beh0lb","colab_type":"text"},"source":["### Estimating the Normalization Constant"]},{"cell_type":"markdown","metadata":{"id":"h_bp6cAhfBLp","colab_type":"text"},"source":["$\\mathcal{C}(\\mu,\\Sigma)\\simeq \\frac{\\mathcal{Z}}{S}\\displaystyle\\sum_{s=1}^{S} m(\\mu,v_s), \\quad $ where $ v_s\\sim\\mathcal(0,\\Sigma)$"]},{"cell_type":"markdown","metadata":{"id":"BzEm5lgbjOH0","colab_type":"text"},"source":["Normailzation constant of the Euclidean normal distribution:<br>\n","$\\mathcal{Z}=\\sqrt{(2\\pi)^{D}|\\Sigma|}$\n"]},{"cell_type":"markdown","metadata":{"id":"9bseY5Sh9vCY","colab_type":"text"},"source":["m-function:\n","\n","$m(\\mu, \\mathbf{v}) = \\sqrt{|\\mathrm{M}(\\mathrm{Exp}_{\\mu}(\\mathbf{v}))|}$"]},{"cell_type":"markdown","metadata":{"id":"foMkQMvviByK","colab_type":"text"},"source":["---\n","\n","**Algorithm 2** The estimation of the normalization constant $\\mathcal{C}(\\mu, \\Sigma)$\n","\n","---\n","\n","**Input**: the given data $\\{x_n\\}_{n=1}^{N}$, the $\\mu$, $\\Sigma$, the number of samples $S$\n","\n","**Output**: the estimated $\\hat{C}(\\mu, \\Sigma)$\n","\n","> **1:** sample $S$ tangent vectors $v_s\\sim\\mathcal(0,\\Sigma)$ on $\\mathcal{T}_\\mu \\mathcal{M}$\n","\n",">  **2:**  map the $v_s$ on $\\mathcal{M}$ as $\\mathbf{x}_s =Exp_\\mu(\\mathbf{v}_s)$, $s=1,...,S$\n","\n","> **3:** compute the normalization constant $\\hat{C}(\\mu, \\Sigma) = \\frac{Z}{S} \\sum_{s=1}^{S} \\sqrt{|(\\text{M}(x_s)|}$"]},{"cell_type":"code","metadata":{"id":"gDMQ9Jz81_sL","colab_type":"code","colab":{}},"source":["# tried both threading and multiprocessing -> mulit-processing is much faster\n","cpus = multiprocessing.cpu_count()\n","\n","@contextmanager\n","def poolcontext(*args, **kwargs):\n","    pool = multiprocessing.Pool(*args, **kwargs)\n","    yield pool\n","    pool.terminate()\n","\n","def calc_m(mu, v):\n","    # M is called to evaluate the metric --> gives the diagonal entries of the metric\n","    # The determinant of a diagonal matrix is the product of the values along the diagonal \n","    # take mu as starting point, vector v as dirction, see where we end up, evaluate the curvature(metric) at the endpoint\n","    p = ExpMap(mu, v) \n","    return M.metric(p).prod().sqrt() #returns scalar\n","    \n","def estimate_C(mu, Sigma, S=1000, v_s=None):\n","    #print(\"Estimating for S: \"+str(S))\n","    # 1.: random samples from a Euclidean normal distribution with zero mean and covariance matrix Sigma\n","    if v_s is None:\n","        mvn = torch.distributions.multivariate_normal.MultivariateNormal(loc=torch.tensor([[0.0,0.0]]), covariance_matrix=Sigma)\n","        v_s = [mvn.sample().reshape(D,) for s in range(S)] #vectors in pytorch are 1-D and have shape: (D)\n","\n","    #2./3.: Compute C using m(mu,v)\n","    # parallelisation\n","    with poolcontext(processes=cpus) as pool:\n","        m_s = pool.map(partial(calc_m, mu), v_s)\n","\n","    Z = math.sqrt(((2*math.pi)**D)*torch.det(Sigma))\n","\n","    C_hat = Z/S * sum(m_s)\n","\n","    return C_hat"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jtmtN2lWfBLu","colab_type":"text"},"source":["###  Eq. 12"]},{"cell_type":"markdown","metadata":{"id":"2a9J5TZtHm1l","colab_type":"text"},"source":["### Objective Function (Eq. 11)"]},{"cell_type":"code","metadata":{"id":"bWXSkp1IHrr4","colab_type":"code","colab":{}},"source":["def calc_objective(mu, Sigma, C=None, datasamples=1000, S=1000):\n","    \n","    LL=0\n","    for n in random.sample(range(0,N),datasamples):\n","        L1 = LogMap(mu,data[n])[0]\n","        L2 = torch.mv(torch.inverse(Sigma), L1)\n","        LL += torch.dot(L1,L2).detach() \n","\n","    if C is None:\n","        C = estimate_C(mu, Sigma, S)\n","           \n","    phi = (LL/(2*datasamples)) + math.log(C)\n","  \n","    return phi"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9i7ZzZuxhTun","colab_type":"text"},"source":["###  Equations for mu"]},{"cell_type":"markdown","metadata":{"id":"cePlDncO2A0i","colab_type":"text"},"source":["Equation 12: $\\nabla_{\\mu} \\phi(\\mu,\\Sigma)=-\\Sigma^{-1}\\left[\\frac{1}{N} \\sum_{n=1}^{N} Log_{\\mu}\\left(\\mathbf{x}_{n}\\right)-\\frac{\\mathcal{Z}}{\\mathcal{C}(\\mu, \\Sigma) \\cdot S} \\sum_{s=1}^{S} m\\left(\\mu, \\mathbf{v}_{s}\\right) \\mathbf{v}_{s}\\right]$"]},{"cell_type":"code","metadata":{"id":"6vVi_9Hs2Ika","colab_type":"code","colab":{}},"source":["def mu_step(mu, Sigma, S, v_s, C, alpha=0.01, data_samples=50):\n","    '''\n","    calculates the gradient for mu and takes one step in this direction\n","    '''\n","    \n","    Z = math.sqrt(((2*math.pi)**D)*torch.det(Sigma))\n","\n","    term1 = (1 / data_samples) * torch.sum(torch.stack([LogMap(mu, data[n])[0] for n in random.sample(range(0, N), data_samples)]), dim=0)\n","    term2 = Z / (C*S) * torch.sum(torch.stack([calc_m(mu, v)*v for v in v_s]), dim = 0)\n","\n","    nabla = term1-term2\n","\n","    mu_p1 = ExpMap(mu, alpha*nabla)\n","\n","    return mu_p1\n","    \n","    \n","    # --- notes ----\n","    #omit denominator because we do steepest descent instead of gradient descent. The covariance matrix is poorly estimated and mu is heavily reliant ont it. \n","    #sgmnv = - torch.inverse(torch.tensor(Sigma,dtype=torch.float))\n","    #nabla = torch.mv(sgmnv, term1-term2)\n","    #---------------"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UyqbkDa7toTH","colab_type":"code","colab":{}},"source":["def init_mu_step(mu, alpha=0.02, data_samples=50):\n","    '''\n","    Calculates the gradient for mu with intrinsic least squares and takes a step in this direction\n","    This is just the same as the first term in Eq. 12\n","    '''\n","    nabla = (1 / data_samples) * torch.sum(torch.stack([LogMap(mu, data[n])[0] for n in random.sample(range(0, N), data_samples)]), dim=0)\n","    mu = ExpMap(mu, alpha*nabla)\n","  \n","    return mu"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b9eChPqx0KoV","colab_type":"text"},"source":["### Equations for Sigma"]},{"cell_type":"markdown","metadata":{"id":"WIBVJuff1vrn","colab_type":"text"},"source":["Equation 13:  $\\nabla_{\\mathbf{A}} \\phi(\\boldsymbol{\\mu}, \\mathbf{\\Sigma})=\\mathbf{A}\\left[\\frac{1}{N} \\sum_{n=1}^{N} \\log _{\\boldsymbol{\\mu}}\\left(\\mathbf{x}_{n}\\right) \\log _{\\boldsymbol{\\mu}}\\left(\\mathbf{x}_{n}\\right)^{\\mathrm{T}}-\\frac{\\mathcal{Z}}{\\mathcal{C}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) \\cdot S} \\sum_{s=1}^{S} m\\left(\\boldsymbol{\\mu}, \\mathbf{v}_{s}\\right) \\mathbf{v}_{s} \\mathbf{v}_{s}^{\\top}\\right]$"]},{"cell_type":"code","metadata":{"id":"mVEd3sCQ0ILQ","colab_type":"code","colab":{}},"source":["def Sigma_step(mu, A, Sigma, S, v_s, C, alpha=0.01, data_samples=50):\n","    '''\n","    calculates the gradient for Sigma and takes one step in this direction\n","    '''\n","    \n","    # equation (12) ------------------------------------------------------------\n","    # --- term1 ---\n","    loglist =[]\n","    for n in random.sample(range(0,N),data_samples):\n","        v = LogMap(mu,data[n])[0]   \n","        loglist.append(torch.ger(v, v))\n","    term1 = (1/data_samples)*torch.sum(torch.stack(loglist),dim=0) #2x2\n","    \n","    # --- term2 ---\n","    Z = math.sqrt(((2*math.pi)**D)*torch.det(Sigma)) # calculate Z w.r.t. Sigma\n","    term2 = (Z / (C*S)) * torch.sum(torch.stack([calc_m(mu, v)*torch.ger(v,v) for v in v_s]), dim = 0) #2x2\n","    \n","    # compute gradient of A (line 7)\n","    grad = torch.mm(A, term1-term2)\n","   \n","    # compute new A (line 8) ---------------------------------------------------                       \n","    A_p1 = A - (alpha * grad)     \n","     \n","    # compute new Sigma (line 9) -----------------------------------------------\n","    Sigma_p1 = torch.inverse(torch.mm(A_p1.transpose(0,-1), A_p1))\n","    \n","    return A_p1,Sigma_p1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wDX0XpfV0RuN","colab_type":"code","colab":{}},"source":["def estimate_Sigma(mu):\n","    '''\n","    Estimate Sigma w.r.t. mu using intrinsic least squares \n","    '''\n","   \n","    mu = torch.tensor([[0.1518, 0.3105]]) #estimated mu\n","    logs_tensor = torch.empty(N, 2) \n","\n","    for n in range(N):\n","        logs_tensor[n] = LogMap(mu, data[n])[0]\n","\n","    Sigma_estimate = sum([torch.ger(logs,logs) for logs in logs_tensor])/(N-1)\n","    \n","    return Sigma_estimate"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lKWmy5Y00WZQ","colab_type":"code","colab":{}},"source":["def calc_A(Sigma):\n","    \n","    #torch.set_printoptions(precision=10)\n","    \n","    '''\n","    Calculates the A for a given Sigma via eigen decomposition\n","    '''\n","   \n","    eig_vals, eig_vecs = torch.symeig(Sigma, eigenvectors=True)\n","    \n","    D_ = torch.tensor([[eig_vals[0], 0.0],[0.0, eig_vals[1]]])\n","    D_half_inv = torch.inverse(D_**0.5)\n","\n","    A_t = torch.mm(eig_vecs, D_half_inv)\n","    A = torch.t(A_t)\n","    \n","    #check\n","    if not torch.all(torch.lt(torch.abs(torch.add(torch.inverse(Sigma), -torch.mm(A_t, A))), 1e-4)):\n","        \n","        print(torch.inverse(Sigma))\n","        print(torch.mm(A_t, A))\n","        \n","        \n","        raise ValueError('Sigma inverse does not merch A_t A')\n","        \n","    return A"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7-N52hxyw2AO","colab_type":"code","outputId":"6a7e291a-edde-4069-a55d-a9896075d7b3","executionInfo":{"status":"error","timestamp":1557752967697,"user_tz":-120,"elapsed":6570,"user":{"displayName":"Tom Haider","photoUrl":"","userId":"04727568713436086022"}},"colab":{"base_uri":"https://localhost:8080/","height":130}},"source":["stop here"],"execution_count":0,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-a96ba3aab008>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    stop here\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"markdown","metadata":{"id":"HKTe9NYj7jKe","colab_type":"text"},"source":["## Optimization runs"]},{"cell_type":"markdown","metadata":{"id":"1N9m7rim21m7","colab_type":"text"},"source":["##### Initializing mu"]},{"cell_type":"code","metadata":{"id":"8ilHWK4hEhRm","colab_type":"code","colab":{}},"source":["#hyperparamters\n","alpha = 0.01\n","data_samples = 50\n","iters = 200\n","verbose = 1\n","ctr = 1\n","\n","# start with mu as arithmetic mean\n","mu = torch.mean(data, dim=0).reshape(1, 2)\n","\n","mu_init = torch.zeros(iters, D)\n","mu_init[0] = mu\n","    \n","while 1:\n","    if ctr >= 0: #replace with stop condition\n","        break\n","        \n","    if ctr > 150:\n","        alpha = 0.005\n","        \n","    mu_p1 = init_mu_step(mu, alpha, data_samples)\n","    \n","    #for saving\n","    mu_init[ctr]= mu_p1\n","    \n","    if not ctr % verbose:\n","        print(\"{}:\".format(ctr))\n","        print(\"mu:\")\n","        print(mu_p1)\n","        \n","        torch.save(mu_init, 'results_MNIST/mu_init.pt')\n","        \n","    #update parameters\n","    mu = mu_p1\n","    ctr += 1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aveiFr3b26CG","colab_type":"text"},"source":["##### Fitting only mu with Constant Sigma"]},{"cell_type":"code","metadata":{"id":"Z5NfWlAkSqPf","colab_type":"code","colab":{}},"source":["#hyperparamters\n","alpha = 0.01\n","S = 500\n","data_samples = 50\n","iters = 50\n","ctr = 1\n","verbose = 1\n","\n","# estimations via intrinsic least squares\n","mu = torch.tensor([[0.1491, 0.3293]]) \n","Sigma = torch.tensor([[ 1.2289, -0.0078],\n","        [-0.0078,  0.0124]])\n","\n","#for saving\n","mu_opti = torch.zeros(iters, D)\n","C_mu_opti = torch.zeros(iters)\n","mu_opti[0]=mu\n","\n","while 1:\n","    if ctr>=0:#replace with stop condition or iters\n","        break\n","    if ctr>=0.9*iters:\n","        alpha = 0.005\n","        data_samples = 1000\n","  \n","    # random sample vectors\n","    mvn = torch.distributions.multivariate_normal.MultivariateNormal(loc=torch.tensor([[0.0,0.0]]), covariance_matrix=Sigma)\n","    v_s = [mvn.sample().reshape(D,) for _ in range(S)]#we use 1-D vectors\n","    \n","    # estimate C\n","    C = estimate_C(mu, Sigma, S, v_s)\n","    \n","    # take Step\n","    mu_p1 = mu_step(mu, Sigma, S, v_s, C, alpha, data_samples)\n","      \n","    #for saving\n","    mu_opti[ctr] = mu_p1\n","    C_mu_opti[ctr-1] = C\n","    \n","    if not ctr % verbose:\n","        print(mu_p1)\n","        torch.save(mu_opti, 'results_MNIST/mu_opti.pt')\n","        torch.save(C_mu_opti, 'results_MNIST/C_mu_opti.pt')\n","\n","    #update parameters\n","    mu = mu_p1\n","    ctr += 1  \n","\n","#save all data\n","#C_mu_opti[ctr-1] = estimate_C(mu, Sigma, S, v_s)\n","#torch.save(mu_opti, 'results_MNIST/mu_opti.pt')\n","#torch.save(C_mu_opti, 'results_MNIST/C_mu_opti.pt')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aBjn2Z6C5T3b","colab_type":"text"},"source":["##### Fitting only Sigma with constant mu"]},{"cell_type":"code","metadata":{"id":"S0nwsgZ6LlW7","colab_type":"code","colab":{}},"source":["#initial values\n","alpha = 0.01\n","S = 500\n","data_samples = 50\n","iters = 100\n","ctr = 1\n","verbose = 1\n","\n","#optimized mu, estimated Sigma\n","mu = torch.tensor([[0.1635, 0.3242]])\n","\n","#Sigma = estimate_Sigma(mu)\n","Sigma = torch.tensor([[ 1.2289, -0.0078],\n","        [-0.0078,  0.0124]])\n","\n","A = calc_A(Sigma)\n","\n","#for saving\n","Sigma_opti = torch.zeros(iters, D, D)\n","C_Sigma_opti = torch.zeros(iters)\n","Sigma_opti[0]=Sigma\n","\n","# while loop for optimisation\n","while 1:\n","    if ctr > iters: #replace with stop condition or iters\n","        break\n","        \n","    if ctr>=0.9*iters:\n","        alpha = 0.005\n","        data_samples = 1000\n","    \n","    # random sample vectors\n","    mvn = torch.distributions.multivariate_normal.MultivariateNormal(loc=torch.tensor([[0.0,0.0]]), covariance_matrix=Sigma)\n","    v_s = [mvn.sample().reshape(D,) for _ in range(S)]   #we use 1-D vectors\n","\n","    #estimate C\n","    C = estimate_C(mu, Sigma, S, v_s)\n","    \n","    #take Step\n","    A_p1,Sigma_p1 = Sigma_step(mu, A, Sigma, S, v_s, C, alpha, data_samples)\n","    \n","    #for saving\n","    Sigma_opti[ctr] = Sigma_p1\n","    C_Sigma_opti[ctr-1] = C\n","    \n","    if not ctr % verbose:\n","        print(\"{}:\".format(ctr))\n","        print(\"Sigma:\")\n","        print(Sigma_p1)\n","        print(\"A:\")\n","        print(A_p1)\n","        \n","        torch.save(Sigma_opti, 'results_MNIST/Sigma_opti.pt')\n","        torch.save(C_Sigma_opti, 'results_MNIST/C_Sigma_opti.pt')\n","        \n","    #update parameters\n","    A,Sigma=A_p1,Sigma_p1\n","    ctr+=1    \n","\n","#save all data\n","C_Sigma_opti[ctr-1] = estimate_C(mu, Sigma, S, v_s)\n","torch.save(Sigma_opti, 'results_MNIST/Sigma_opti')\n","torch.save(C_Sigma_opti, 'results_MNIST/C_Sigma_opti')    \n","    \n","# --------------------------------------------------------------------------\n","# Torch notes:\n","# easiest way to do vectors in pytorch is in 1-D -> shape (D) \n","# other way are matrices, that are 2-D : shape(D,1) D rows, 1 column\n","# torch.ger(v,v) is just the same as v*v^T -> outer product of two vectors\n","# torch.mm - matrix multiplication (c_ij = a_i1*b_i1 + ... + a_im*b_im)\n","# A*B - elementwise muliplication of two matrices (Hadamard product)-- mathematical notation would be: A ∘ B"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vZxCFAXySxHz","colab_type":"text"},"source":["##### Combine mu and Sigma"]},{"cell_type":"code","metadata":{"id":"xjXHekMXS1Wv","colab_type":"code","outputId":"d6389440-45ff-4003-e93e-3aefba9a2867","executionInfo":{"status":"ok","timestamp":1557776753867,"user_tz":-120,"elapsed":23301112,"user":{"displayName":"Tom Haider","photoUrl":"","userId":"04727568713436086022"}},"colab":{"base_uri":"https://localhost:8080/","height":7531}},"source":["# initialise values\n","alpha = 0.01\n","S=1000\n","data_samples = 50\n","iters = 100\n","ctr= 60\n","verbose = 1\n","\n","#estimated mu, estimated Sigma, derived A\n","mu = torch.tensor([[0.1491, 0.3293]]) \n","\n","#Sigma = estimate_Sigma(mu)\n","Sigma = torch.tensor([[ 1.2289, -0.0078],\n","        [-0.0078,  0.0124]])\n","\n","#after 59 steps\n","mu = torch.tensor([0.1452, 0.3245])\n","Sigma = torch.tensor([[ 0.5471, -0.0011],\n","        [-0.0011,  0.0113]])\n","\n","print(Sigma)\n","\n","A = calc_A(Sigma)\n","\n","mu_merged  = torch.load('results_MNIST/mu_ad_steps.pt')\n","Sigma_merged = torch.load('results_MNIST/Sigma_ad_steps.pt')\n","C_merged = torch.load('results_MNIST/C_ad_steps.pt')\n","#mu_merged[0]=mu\n","#Sigma_merged[0]=Sigma\n","\n","\n","# while loop for optimisation\n","while 1:\n","    if ctr >= iters: #replace with stop condition\n","        break\n","        \n","    if ctr>0:\n","        alpha = 0.1\n","    \n","    if ctr>=20:\n","        alpha = 0.05    \n","    \n","    if ctr>=40:\n","        alpha = 0.025   \n","    \n","    if ctr>=60:\n","        alpha = 0.01\n","    \n","    if ctr>=80:\n","        aplpha = 0.005\n","        data_samples=N\n","    \n","    print(\"current alpha: {}, sample_size:{}\".format(alpha, data_samples))\n","    \n","    \n","    t1 = perf_counter() # see how long we need\n","    \n","    \n","    # mu -----------------------------------------------------------------------\n","    # random sample vectors\n","    mvn = torch.distributions.multivariate_normal.MultivariateNormal(loc=torch.tensor([[0.0,0.0]]), covariance_matrix=Sigma)\n","    v_s = [mvn.sample().reshape(D,) for _ in range(S)]#we use 1-D vectors\n","    # estimate C\n","    C = estimate_C(mu, Sigma, S, v_s)\n","    # take Step\n","    mu_p1 = mu_step(mu, Sigma, S, v_s, C, alpha, data_samples)\n","    \n","    # Sigma --------------------------------------------------------------------\n","    # random sample vectors\n","    mvn = torch.distributions.multivariate_normal.MultivariateNormal(loc=torch.tensor([[0.0,0.0]]), covariance_matrix=Sigma)\n","    v_s = [mvn.sample().reshape(D,) for s in range(S)]#we use 1-D vectors\n","    #estimate C\n","    C = estimate_C(mu_p1, Sigma, S, v_s)\n","    #take Step\n","    A_p1,Sigma_p1 = Sigma_step(mu, A, Sigma, S, v_s, C, alpha, data_samples)\n","    \n","    \n","    #for saving\n","    #for saving\n","    mu_merged[ctr] = mu_p1\n","    Sigma_merged[ctr] = Sigma_p1\n","    C_merged[ctr-1] = C\n","    \n","    if not ctr % verbose:\n","        print(\"{}:\".format(ctr))\n","        print(\"mu:\")\n","        print(mu_p1)\n","        print(\"Sigma:\")\n","        print(Sigma_p1)\n","        print(\"A:\")\n","        print(A_p1)\n","        \n","        torch.save(mu_merged, 'results_MNIST/mu_ad_steps.pt')\n","        torch.save(Sigma_merged, 'results_MNIST/Sigma_ad_steps.pt')\n","        torch.save(C_merged, 'results_MNIST/C_ad_steps.pt')\n","        \n","        t2 = perf_counter()-t1\n","        print(\"current time per step: {:3.2f}\".format(t2))\n","        \n","    #update parameters\n","    mu = mu_p1\n","    A = A_p1\n","    Sigma = Sigma_p1\n","    ctr+=1    "],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor([[ 0.5471, -0.0011],\n","        [-0.0011,  0.0113]])\n","current alpha: 0.01, sample_size:50\n","60:\n","mu:\n","tensor([[0.1431, 0.3246]])\n","Sigma:\n","tensor([[ 0.5494, -0.0012],\n","        [-0.0012,  0.0113]])\n","A:\n","tensor([[-2.0447e-02, -9.4088e+00],\n","        [-1.3491e+00,  2.6071e-03]])\n","current time per step: 373.57\n","current alpha: 0.01, sample_size:50\n","61:\n","mu:\n","tensor([[0.1410, 0.3248]])\n","Sigma:\n","tensor([[ 0.5505, -0.0011],\n","        [-0.0011,  0.0113]])\n","A:\n","tensor([[-1.9733e-02, -9.4100e+00],\n","        [-1.3477e+00,  2.7070e-03]])\n","current time per step: 383.11\n","current alpha: 0.01, sample_size:50\n","62:\n","mu:\n","tensor([[0.1406, 0.3249]])\n","Sigma:\n","tensor([[ 0.5509, -0.0011],\n","        [-0.0011,  0.0113]])\n","A:\n","tensor([[-1.9233e-02, -9.4111e+00],\n","        [-1.3473e+00,  2.7780e-03]])\n","current time per step: 388.64\n","current alpha: 0.01, sample_size:50\n","63:\n","mu:\n","tensor([[0.1372, 0.3248]])\n","Sigma:\n","tensor([[ 0.5479, -0.0012],\n","        [-0.0012,  0.0113]])\n","A:\n","tensor([[-2.0447e-02, -9.4129e+00],\n","        [-1.3510e+00,  2.6123e-03]])\n","current time per step: 375.64\n","current alpha: 0.01, sample_size:50\n","64:\n","mu:\n","tensor([[0.1354, 0.3246]])\n","Sigma:\n","tensor([[ 0.5467, -0.0011],\n","        [-0.0011,  0.0113]])\n","A:\n","tensor([[-1.8959e-02, -9.4151e+00],\n","        [-1.3525e+00,  2.8297e-03]])\n","current time per step: 375.89\n","current alpha: 0.01, sample_size:50\n","65:\n","mu:\n","tensor([[0.1365, 0.3245]])\n","Sigma:\n","tensor([[ 0.5455, -0.0010],\n","        [-0.0010,  0.0113]])\n","A:\n","tensor([[-1.6955e-02, -9.4168e+00],\n","        [-1.3539e+00,  3.1209e-03]])\n","current time per step: 367.92\n","current alpha: 0.01, sample_size:50\n","66:\n","mu:\n","tensor([[0.1382, 0.3246]])\n","Sigma:\n","tensor([[ 0.5459, -0.0009],\n","        [-0.0009,  0.0113]])\n","A:\n","tensor([[-1.5232e-02, -9.4190e+00],\n","        [-1.3535e+00,  3.3685e-03]])\n","current time per step: 376.36\n","current alpha: 0.01, sample_size:50\n","67:\n","mu:\n","tensor([[0.1363, 0.3247]])\n","Sigma:\n","tensor([[ 0.5483, -0.0009],\n","        [-0.0009,  0.0113]])\n","A:\n","tensor([[-1.5547e-02, -9.4207e+00],\n","        [-1.3504e+00,  3.3189e-03]])\n","current time per step: 381.81\n","current alpha: 0.01, sample_size:50\n","68:\n","mu:\n","tensor([[0.1353, 0.3248]])\n","Sigma:\n","tensor([[ 0.5474, -0.0010],\n","        [-0.0010,  0.0113]])\n","A:\n","tensor([[-1.7048e-02, -9.4203e+00],\n","        [-1.3516e+00,  3.1058e-03]])\n","current time per step: 380.81\n","current alpha: 0.01, sample_size:50\n","69:\n","mu:\n","tensor([[0.1357, 0.3246]])\n","Sigma:\n","tensor([[ 0.5493, -0.0009],\n","        [-0.0009,  0.0113]])\n","A:\n","tensor([[-1.5437e-02, -9.4219e+00],\n","        [-1.3492e+00,  3.3329e-03]])\n","current time per step: 394.58\n","current alpha: 0.01, sample_size:50\n","70:\n","mu:\n","tensor([[0.1336, 0.3244]])\n","Sigma:\n","tensor([[ 0.5476, -0.0010],\n","        [-0.0010,  0.0113]])\n","A:\n","tensor([[-1.7463e-02, -9.4238e+00],\n","        [-1.3513e+00,  3.0471e-03]])\n","current time per step: 396.71\n","current alpha: 0.01, sample_size:50\n","71:\n","mu:\n","tensor([[0.1293, 0.3246]])\n","Sigma:\n","tensor([[ 0.5506, -0.0011],\n","        [-0.0011,  0.0113]])\n","A:\n","tensor([[-1.8921e-02, -9.4255e+00],\n","        [-1.3477e+00,  2.8317e-03]])\n","current time per step: 378.94\n","current alpha: 0.01, sample_size:50\n","72:\n","mu:\n","tensor([[0.1309, 0.3248]])\n","Sigma:\n","tensor([[ 0.5521, -0.0012],\n","        [-0.0012,  0.0113]])\n","A:\n","tensor([[-2.0096e-02, -9.4267e+00],\n","        [-1.3459e+00,  2.6603e-03]])\n","current time per step: 367.82\n","current alpha: 0.01, sample_size:50\n","73:\n","mu:\n","tensor([[0.1308, 0.3248]])\n","Sigma:\n","tensor([[ 0.5538, -0.0011],\n","        [-0.0011,  0.0113]])\n","A:\n","tensor([[-1.9059e-02, -9.4272e+00],\n","        [-1.3437e+00,  2.8041e-03]])\n","current time per step: 378.17\n","current alpha: 0.01, sample_size:50\n","74:\n","mu:\n","tensor([[0.1316, 0.3248]])\n","Sigma:\n","tensor([[ 0.5527, -0.0011],\n","        [-0.0011,  0.0112]])\n","A:\n","tensor([[-1.8667e-02, -9.4290e+00],\n","        [-1.3451e+00,  2.8633e-03]])\n","current time per step: 371.10\n","current alpha: 0.01, sample_size:50\n","75:\n","mu:\n","tensor([[0.1294, 0.3248]])\n","Sigma:\n","tensor([[ 0.5518, -0.0011],\n","        [-0.0011,  0.0112]])\n","A:\n","tensor([[-1.8443e-02, -9.4310e+00],\n","        [-1.3463e+00,  2.8980e-03]])\n","current time per step: 402.43\n","current alpha: 0.01, sample_size:50\n","76:\n","mu:\n","tensor([[0.1289, 0.3246]])\n","Sigma:\n","tensor([[ 0.5533, -0.0010],\n","        [-0.0010,  0.0112]])\n","A:\n","tensor([[-1.7648e-02, -9.4327e+00],\n","        [-1.3444e+00,  3.0084e-03]])\n","current time per step: 378.46\n","current alpha: 0.01, sample_size:50\n","77:\n","mu:\n","tensor([[0.1263, 0.3246]])\n","Sigma:\n","tensor([[ 0.5530, -0.0011],\n","        [-0.0011,  0.0112]])\n","A:\n","tensor([[-1.9310e-02, -9.4346e+00],\n","        [-1.3447e+00,  2.7728e-03]])\n","current time per step: 379.72\n","current alpha: 0.01, sample_size:50\n","78:\n","mu:\n","tensor([[0.1244, 0.3246]])\n","Sigma:\n","tensor([[ 0.5529, -0.0009],\n","        [-0.0009,  0.0112]])\n","A:\n","tensor([[-1.6258e-02, -9.4363e+00],\n","        [-1.3448e+00,  3.2085e-03]])\n","current time per step: 403.28\n","current alpha: 0.01, sample_size:50\n","79:\n","mu:\n","tensor([[0.1229, 0.3244]])\n","Sigma:\n","tensor([[ 0.5557, -0.0008],\n","        [-0.0008,  0.0112]])\n","A:\n","tensor([[-1.4516e-02, -9.4381e+00],\n","        [-1.3414e+00,  3.4515e-03]])\n","current time per step: 371.75\n","current alpha: 0.01, sample_size:1000\n","80:\n","mu:\n","tensor([[0.1240, 0.3246]])\n","Sigma:\n","tensor([[ 0.5536, -0.0007],\n","        [-0.0007,  0.0112]])\n","A:\n","tensor([[-1.1701e-02, -9.4397e+00],\n","        [-1.3440e+00,  3.8561e-03]])\n","current time per step: 789.16\n","current alpha: 0.01, sample_size:1000\n","81:\n","mu:\n","tensor([[0.1250, 0.3245]])\n","Sigma:\n","tensor([[ 0.5538, -0.0007],\n","        [-0.0007,  0.0112]])\n","A:\n","tensor([[-1.1912e-02, -9.4413e+00],\n","        [-1.3438e+00,  3.8266e-03]])\n","current time per step: 792.98\n","current alpha: 0.01, sample_size:1000\n","82:\n","mu:\n","tensor([[0.1257, 0.3245]])\n","Sigma:\n","tensor([[ 5.5476e-01, -5.0143e-04],\n","        [-5.0143e-04,  1.1215e-02]])\n","A:\n","tensor([[-9.1353e-03, -9.4430e+00],\n","        [-1.3426e+00,  4.2209e-03]])\n","current time per step: 786.18\n","current alpha: 0.01, sample_size:1000\n","83:\n","mu:\n","tensor([[0.1261, 0.3244]])\n","Sigma:\n","tensor([[ 0.5539, -0.0007],\n","        [-0.0007,  0.0112]])\n","A:\n","tensor([[-1.1730e-02, -9.4444e+00],\n","        [-1.3437e+00,  3.8536e-03]])\n","current time per step: 797.07\n","current alpha: 0.01, sample_size:1000\n","84:\n","mu:\n","tensor([[0.1254, 0.3244]])\n","Sigma:\n","tensor([[ 0.5542, -0.0007],\n","        [-0.0007,  0.0112]])\n","A:\n","tensor([[-1.1880e-02, -9.4460e+00],\n","        [-1.3432e+00,  3.8325e-03]])\n","current time per step: 795.29\n","current alpha: 0.01, sample_size:1000\n","85:\n","mu:\n","tensor([[0.1262, 0.3242]])\n","Sigma:\n","tensor([[ 0.5545, -0.0007],\n","        [-0.0007,  0.0112]])\n","A:\n","tensor([[-1.2859e-02, -9.4477e+00],\n","        [-1.3429e+00,  3.6935e-03]])\n","current time per step: 794.44\n","current alpha: 0.01, sample_size:1000\n","86:\n","mu:\n","tensor([[0.1270, 0.3241]])\n","Sigma:\n","tensor([[ 0.5533, -0.0007],\n","        [-0.0007,  0.0112]])\n","A:\n","tensor([[-1.1661e-02, -9.4490e+00],\n","        [-1.3443e+00,  3.8662e-03]])\n","current time per step: 790.80\n","current alpha: 0.01, sample_size:1000\n","87:\n","mu:\n","tensor([[0.1271, 0.3242]])\n","Sigma:\n","tensor([[ 5.5338e-01, -4.2212e-04],\n","        [-4.2212e-04,  1.1197e-02]])\n","A:\n","tensor([[-7.8362e-03, -9.4505e+00],\n","        [-1.3443e+00,  4.4109e-03]])\n","current time per step: 800.02\n","current alpha: 0.01, sample_size:1000\n","88:\n","mu:\n","tensor([[0.1274, 0.3243]])\n","Sigma:\n","tensor([[ 5.5264e-01, -2.7394e-04],\n","        [-2.7394e-04,  1.1193e-02]])\n","A:\n","tensor([[-5.3634e-03, -9.4521e+00],\n","        [-1.3452e+00,  4.7642e-03]])\n","current time per step: 795.02\n","current alpha: 0.01, sample_size:1000\n","89:\n","mu:\n","tensor([[0.1283, 0.3244]])\n","Sigma:\n","tensor([[ 5.5168e-01, -1.6226e-04],\n","        [-1.6226e-04,  1.1189e-02]])\n","A:\n","tensor([[-3.4971e-03, -9.4540e+00],\n","        [-1.3463e+00,  5.0314e-03]])\n","current time per step: 784.44\n","current alpha: 0.01, sample_size:1000\n","90:\n","mu:\n","tensor([[0.1270, 0.3242]])\n","Sigma:\n","tensor([[ 5.5120e-01, -4.9017e-05],\n","        [-4.9017e-05,  1.1185e-02]])\n","A:\n","tensor([[-1.5963e-03, -9.4555e+00],\n","        [-1.3469e+00,  5.3031e-03]])\n","current time per step: 781.90\n","current alpha: 0.01, sample_size:1000\n","91:\n","mu:\n","tensor([[0.1284, 0.3243]])\n","Sigma:\n","tensor([[ 5.5128e-01, -6.9494e-06],\n","        [-6.9494e-06,  1.1181e-02]])\n","A:\n","tensor([[-8.8893e-04, -9.4571e+00],\n","        [-1.3468e+00,  5.4047e-03]])\n","current time per step: 759.74\n","current alpha: 0.01, sample_size:1000\n","92:\n","mu:\n","tensor([[0.1276, 0.3243]])\n","Sigma:\n","tensor([[5.5082e-01, 5.5161e-05],\n","        [5.5161e-05, 1.1177e-02]])\n","A:\n","tensor([[ 1.5599e-04, -9.4587e+00],\n","        [-1.3474e+00,  5.5545e-03]])\n","current time per step: 775.01\n","current alpha: 0.01, sample_size:1000\n","93:\n","mu:\n","tensor([[0.1270, 0.3243]])\n","Sigma:\n","tensor([[ 5.5087e-01, -2.9056e-05],\n","        [-2.9056e-05,  1.1173e-02]])\n","A:\n","tensor([[-1.2614e-03, -9.4603e+00],\n","        [-1.3473e+00,  5.3536e-03]])\n","current time per step: 791.02\n","current alpha: 0.01, sample_size:1000\n","94:\n","mu:\n","tensor([[0.1277, 0.3244]])\n","Sigma:\n","tensor([[ 5.5208e-01, -1.7588e-05],\n","        [-1.7588e-05,  1.1170e-02]])\n","A:\n","tensor([[-1.0670e-03, -9.4617e+00],\n","        [-1.3459e+00,  5.3818e-03]])\n","current time per step: 781.34\n","current alpha: 0.01, sample_size:1000\n","95:\n","mu:\n","tensor([[0.1272, 0.3244]])\n","Sigma:\n","tensor([[5.4972e-01, 3.6655e-05],\n","        [3.6655e-05, 1.1167e-02]])\n","A:\n","tensor([[-1.5473e-04, -9.4629e+00],\n","        [-1.3487e+00,  5.5126e-03]])\n","current time per step: 798.97\n","current alpha: 0.01, sample_size:1000\n","96:\n","mu:\n","tensor([[0.1272, 0.3246]])\n","Sigma:\n","tensor([[5.5047e-01, 1.1095e-04],\n","        [1.1095e-04, 1.1164e-02]])\n","A:\n","tensor([[ 1.0970e-03, -9.4644e+00],\n","        [-1.3478e+00,  5.6919e-03]])\n","current time per step: 775.94\n","current alpha: 0.01, sample_size:1000\n","97:\n","mu:\n","tensor([[0.1271, 0.3246]])\n","Sigma:\n","tensor([[5.5001e-01, 7.3672e-05],\n","        [7.3672e-05, 1.1161e-02]])\n","A:\n","tensor([[ 4.6973e-04, -9.4658e+00],\n","        [-1.3484e+00,  5.6033e-03]])\n","current time per step: 775.53\n","current alpha: 0.01, sample_size:1000\n","98:\n","mu:\n","tensor([[0.1276, 0.3247]])\n","Sigma:\n","tensor([[5.5035e-01, 1.2129e-04],\n","        [1.2129e-04, 1.1157e-02]])\n","A:\n","tensor([[ 1.2723e-03, -9.4674e+00],\n","        [-1.3480e+00,  5.7186e-03]])\n","current time per step: 768.37\n","current alpha: 0.01, sample_size:1000\n","99:\n","mu:\n","tensor([[0.1280, 0.3246]])\n","Sigma:\n","tensor([[5.4962e-01, 8.6534e-05],\n","        [8.6534e-05, 1.1153e-02]])\n","A:\n","tensor([[ 6.8793e-04, -9.4690e+00],\n","        [-1.3489e+00,  5.6363e-03]])\n","current time per step: 740.35\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"T23Q4S5V-kch","colab_type":"code","colab":{}},"source":["mu_merged= torch.load('results_MNIST/mu_ad_steps.pt')\n","C_merged[ctr-1] = estimate_C(mu_merged[-1].reshape(1,2), Sigma_merged[-1], S, v_s)\n","torch.save(C_merged, 'results_MNIST/C_ad_steps.pt')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YVZN1KyXZqaP","colab_type":"code","outputId":"0b2444ac-8768-4e76-d46d-8786e07a643a","executionInfo":{"status":"ok","timestamp":1557778664870,"user_tz":-120,"elapsed":929,"user":{"displayName":"Tom Haider","photoUrl":"","userId":"04727568713436086022"}},"colab":{"base_uri":"https://localhost:8080/","height":5100}},"source":["torch.load('results_MNIST/Sigma_ad_steps.pt')\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 1.2289e+00, -7.8000e-03],\n","         [-7.8000e-03,  1.2400e-02]],\n","\n","        [[ 8.2857e-01, -6.6626e-03],\n","         [-6.6626e-03,  1.2358e-02]],\n","\n","        [[ 7.0677e-01, -3.6605e-03],\n","         [-3.6605e-03,  1.2277e-02]],\n","\n","        [[ 6.3227e-01, -6.2028e-03],\n","         [-6.2028e-03,  1.2277e-02]],\n","\n","        [[ 5.9995e-01, -7.9561e-03],\n","         [-7.9561e-03,  1.2293e-02]],\n","\n","        [[ 5.6177e-01, -7.3107e-03],\n","         [-7.3107e-03,  1.2227e-02]],\n","\n","        [[ 5.7630e-01, -6.2754e-03],\n","         [-6.2754e-03,  1.2143e-02]],\n","\n","        [[ 5.6431e-01, -9.3543e-03],\n","         [-9.3543e-03,  1.2211e-02]],\n","\n","        [[ 5.8292e-01, -1.2617e-02],\n","         [-1.2617e-02,  1.2285e-02]],\n","\n","        [[ 5.9051e-01, -1.1690e-02],\n","         [-1.1690e-02,  1.2190e-02]],\n","\n","        [[ 5.7469e-01, -1.0926e-02],\n","         [-1.0926e-02,  1.2184e-02]],\n","\n","        [[ 5.5923e-01, -1.0447e-02],\n","         [-1.0447e-02,  1.2124e-02]],\n","\n","        [[ 5.3653e-01, -9.1445e-03],\n","         [-9.1445e-03,  1.2149e-02]],\n","\n","        [[ 5.1425e-01, -1.0429e-02],\n","         [-1.0429e-02,  1.2162e-02]],\n","\n","        [[ 5.1795e-01, -9.6898e-03],\n","         [-9.6898e-03,  1.2088e-02]],\n","\n","        [[ 5.4984e-01, -8.8903e-03],\n","         [-8.8903e-03,  1.2019e-02]],\n","\n","        [[ 5.5072e-01, -6.7454e-03],\n","         [-6.7454e-03,  1.1916e-02]],\n","\n","        [[ 5.4472e-01, -7.3961e-03],\n","         [-7.3961e-03,  1.1971e-02]],\n","\n","        [[ 5.2680e-01, -7.1155e-03],\n","         [-7.1155e-03,  1.1929e-02]],\n","\n","        [[ 5.3065e-01, -9.9788e-03],\n","         [-9.9788e-03,  1.2064e-02]],\n","\n","        [[ 5.3705e-01, -1.1145e-02],\n","         [-1.1145e-02,  1.2084e-02]],\n","\n","        [[ 5.4649e-01, -1.0914e-02],\n","         [-1.0914e-02,  1.2058e-02]],\n","\n","        [[ 5.4911e-01, -1.1214e-02],\n","         [-1.1214e-02,  1.2049e-02]],\n","\n","        [[ 5.2894e-01, -1.0399e-02],\n","         [-1.0399e-02,  1.2005e-02]],\n","\n","        [[ 5.2039e-01, -9.3585e-03],\n","         [-9.3585e-03,  1.1957e-02]],\n","\n","        [[ 5.2893e-01, -9.4731e-03],\n","         [-9.4731e-03,  1.1935e-02]],\n","\n","        [[ 5.1109e-01, -1.0151e-02],\n","         [-1.0151e-02,  1.1957e-02]],\n","\n","        [[ 5.1871e-01, -1.0108e-02],\n","         [-1.0108e-02,  1.1927e-02]],\n","\n","        [[ 5.1945e-01, -9.9622e-03],\n","         [-9.9622e-03,  1.1895e-02]],\n","\n","        [[ 5.1564e-01, -9.7212e-03],\n","         [-9.7212e-03,  1.1867e-02]],\n","\n","        [[ 5.1728e-01, -9.3848e-03],\n","         [-9.3848e-03,  1.1837e-02]],\n","\n","        [[ 5.2354e-01, -8.9869e-03],\n","         [-8.9869e-03,  1.1807e-02]],\n","\n","        [[ 5.2336e-01, -8.0356e-03],\n","         [-8.0356e-03,  1.1755e-02]],\n","\n","        [[ 5.3377e-01, -7.6523e-03],\n","         [-7.6523e-03,  1.1717e-02]],\n","\n","        [[ 5.1731e-01, -6.9493e-03],\n","         [-6.9493e-03,  1.1678e-02]],\n","\n","        [[ 5.1521e-01, -6.6585e-03],\n","         [-6.6585e-03,  1.1648e-02]],\n","\n","        [[ 5.2698e-01, -5.5666e-03],\n","         [-5.5666e-03,  1.1597e-02]],\n","\n","        [[ 5.3665e-01, -4.6093e-03],\n","         [-4.6093e-03,  1.1559e-02]],\n","\n","        [[ 5.3395e-01, -4.2816e-03],\n","         [-4.2816e-03,  1.1530e-02]],\n","\n","        [[ 5.3509e-01, -3.5766e-03],\n","         [-3.5766e-03,  1.1499e-02]],\n","\n","        [[ 5.3704e-01, -3.3532e-03],\n","         [-3.3532e-03,  1.1490e-02]],\n","\n","        [[ 5.3478e-01, -3.0960e-03],\n","         [-3.0960e-03,  1.1476e-02]],\n","\n","        [[ 5.3817e-01, -3.1364e-03],\n","         [-3.1364e-03,  1.1469e-02]],\n","\n","        [[ 5.3527e-01, -2.6891e-03],\n","         [-2.6891e-03,  1.1455e-02]],\n","\n","        [[ 5.3229e-01, -2.5324e-03],\n","         [-2.5324e-03,  1.1444e-02]],\n","\n","        [[ 5.4098e-01, -2.6639e-03],\n","         [-2.6639e-03,  1.1432e-02]],\n","\n","        [[ 5.4706e-01, -2.6164e-03],\n","         [-2.6164e-03,  1.1422e-02]],\n","\n","        [[ 5.4330e-01, -2.6408e-03],\n","         [-2.6408e-03,  1.1421e-02]],\n","\n","        [[ 5.3432e-01, -2.6367e-03],\n","         [-2.6367e-03,  1.1411e-02]],\n","\n","        [[ 5.4341e-01, -2.3945e-03],\n","         [-2.3945e-03,  1.1396e-02]],\n","\n","        [[ 5.4009e-01, -2.3742e-03],\n","         [-2.3742e-03,  1.1385e-02]],\n","\n","        [[ 5.3522e-01, -2.1859e-03],\n","         [-2.1859e-03,  1.1373e-02]],\n","\n","        [[ 5.4188e-01, -2.4100e-03],\n","         [-2.4100e-03,  1.1365e-02]],\n","\n","        [[ 5.4767e-01, -2.0736e-03],\n","         [-2.0736e-03,  1.1351e-02]],\n","\n","        [[ 5.4588e-01, -1.7411e-03],\n","         [-1.7411e-03,  1.1340e-02]],\n","\n","        [[ 5.4808e-01, -1.4067e-03],\n","         [-1.4067e-03,  1.1334e-02]],\n","\n","        [[ 5.5031e-01, -1.2088e-03],\n","         [-1.2088e-03,  1.1323e-02]],\n","\n","        [[ 5.5237e-01, -1.3446e-03],\n","         [-1.3446e-03,  1.1311e-02]],\n","\n","        [[ 5.4573e-01, -1.2761e-03],\n","         [-1.2761e-03,  1.1301e-02]],\n","\n","        [[ 5.4711e-01, -1.0518e-03],\n","         [-1.0518e-03,  1.1289e-02]],\n","\n","        [[ 5.4943e-01, -1.1722e-03],\n","         [-1.1722e-03,  1.1299e-02]],\n","\n","        [[ 5.5053e-01, -1.1318e-03],\n","         [-1.1318e-03,  1.1296e-02]],\n","\n","        [[ 5.5085e-01, -1.1025e-03],\n","         [-1.1025e-03,  1.1293e-02]],\n","\n","        [[ 5.4786e-01, -1.1683e-03],\n","         [-1.1683e-03,  1.1289e-02]],\n","\n","        [[ 5.4667e-01, -1.0772e-03],\n","         [-1.0772e-03,  1.1283e-02]],\n","\n","        [[ 5.4555e-01, -9.5624e-04],\n","         [-9.5624e-04,  1.1279e-02]],\n","\n","        [[ 5.4589e-01, -8.5475e-04],\n","         [-8.5475e-04,  1.1273e-02]],\n","\n","        [[ 5.4833e-01, -8.7725e-04],\n","         [-8.7725e-04,  1.1269e-02]],\n","\n","        [[ 5.4737e-01, -9.6465e-04],\n","         [-9.6465e-04,  1.1270e-02]],\n","\n","        [[ 5.4934e-01, -8.7224e-04],\n","         [-8.7224e-04,  1.1266e-02]],\n","\n","        [[ 5.4762e-01, -9.8935e-04],\n","         [-9.8935e-04,  1.1262e-02]],\n","\n","        [[ 5.5057e-01, -1.0816e-03],\n","         [-1.0816e-03,  1.1258e-02]],\n","\n","        [[ 5.5207e-01, -1.1547e-03],\n","         [-1.1547e-03,  1.1256e-02]],\n","\n","        [[ 5.5382e-01, -1.0962e-03],\n","         [-1.0962e-03,  1.1254e-02]],\n","\n","        [[ 5.5267e-01, -1.0702e-03],\n","         [-1.0702e-03,  1.1250e-02]],\n","\n","        [[ 5.5175e-01, -1.0548e-03],\n","         [-1.0548e-03,  1.1245e-02]],\n","\n","        [[ 5.5329e-01, -1.0101e-03],\n","         [-1.0101e-03,  1.1241e-02]],\n","\n","        [[ 5.5301e-01, -1.1087e-03],\n","         [-1.1087e-03,  1.1237e-02]],\n","\n","        [[ 5.5295e-01, -9.2587e-04],\n","         [-9.2587e-04,  1.1232e-02]],\n","\n","        [[ 5.5572e-01, -8.2585e-04],\n","         [-8.2585e-04,  1.1227e-02]],\n","\n","        [[ 5.5361e-01, -6.5405e-04],\n","         [-6.5405e-04,  1.1223e-02]],\n","\n","        [[ 5.5375e-01, -6.6670e-04],\n","         [-6.6670e-04,  1.1219e-02]],\n","\n","        [[ 5.5476e-01, -5.0143e-04],\n","         [-5.0143e-04,  1.1215e-02]],\n","\n","        [[ 5.5389e-01, -6.5580e-04],\n","         [-6.5580e-04,  1.1212e-02]],\n","\n","        [[ 5.5425e-01, -6.6506e-04],\n","         [-6.6506e-04,  1.1208e-02]],\n","\n","        [[ 5.5448e-01, -7.2388e-04],\n","         [-7.2388e-04,  1.1204e-02]],\n","\n","        [[ 5.5334e-01, -6.5068e-04],\n","         [-6.5068e-04,  1.1201e-02]],\n","\n","        [[ 5.5338e-01, -4.2212e-04],\n","         [-4.2212e-04,  1.1197e-02]],\n","\n","        [[ 5.5264e-01, -2.7394e-04],\n","         [-2.7394e-04,  1.1193e-02]],\n","\n","        [[ 5.5168e-01, -1.6226e-04],\n","         [-1.6226e-04,  1.1189e-02]],\n","\n","        [[ 5.5120e-01, -4.9017e-05],\n","         [-4.9017e-05,  1.1185e-02]],\n","\n","        [[ 5.5128e-01, -6.9494e-06],\n","         [-6.9494e-06,  1.1181e-02]],\n","\n","        [[ 5.5082e-01,  5.5161e-05],\n","         [ 5.5161e-05,  1.1177e-02]],\n","\n","        [[ 5.5087e-01, -2.9056e-05],\n","         [-2.9056e-05,  1.1173e-02]],\n","\n","        [[ 5.5208e-01, -1.7588e-05],\n","         [-1.7588e-05,  1.1170e-02]],\n","\n","        [[ 5.4972e-01,  3.6655e-05],\n","         [ 3.6655e-05,  1.1167e-02]],\n","\n","        [[ 5.5047e-01,  1.1095e-04],\n","         [ 1.1095e-04,  1.1164e-02]],\n","\n","        [[ 5.5001e-01,  7.3672e-05],\n","         [ 7.3672e-05,  1.1161e-02]],\n","\n","        [[ 5.5035e-01,  1.2129e-04],\n","         [ 1.2129e-04,  1.1157e-02]],\n","\n","        [[ 5.4962e-01,  8.6534e-05],\n","         [ 8.6534e-05,  1.1153e-02]]])"]},"metadata":{"tags":[]},"execution_count":84}]},{"cell_type":"code","metadata":{"id":"-fGrhmeCc7O_","colab_type":"code","colab":{}},"source":["mu = torch.load('results_MNIST/mu_merged.pt')\n","Sigma = torch.load('results_MNIST/Sigma_merged.pt')\n","C = torch.load('results_MNIST/C_merged.pt')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"niedh5tbyq96","colab_type":"code","colab":{}},"source":["for i in range(100, len(mu)):\n","    print(i)\n","    phi.append(calc_objective(mu[i].reshape(1,2), Sigma[i], C[i], datasamples=1000, S=500))\n","    print(phi[i])\n","    torch.save(phi,'results_MNIST/phi_merged.pt')\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1fc-IyB60cvW","colab_type":"code","colab":{}},"source":["# estimations via intrinsic least squares\n","mu = torch.tensor([[0.1491, 0.3293]]) \n","Sigma = torch.tensor([[ 1.2289, -0.0078],\n","        [-0.0078,  0.0124]])\n","\n","S_save=[]\n","\n","for S in range(1,3002,100):\n","    print(S)\n","    S_save.append(estimate_C(mu, Sigma, S))\n","    torch.save(S_save, 'results_MNIST/S_3000.pt')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gYDuraff9krz","colab_type":"code","outputId":"22ad9b93-821e-417c-ccaa-e2d0a905d8c5","executionInfo":{"status":"ok","timestamp":1557753313685,"user_tz":-120,"elapsed":1011,"user":{"displayName":"Tom Haider","photoUrl":"","userId":"04727568713436086022"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["torch.load('results_MNIST/Sigma_ad_steps.pt')[59]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.5471, -0.0011],\n","        [-0.0011,  0.0113]])"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"Gez9wJ5a-GWe","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}